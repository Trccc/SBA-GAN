{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import range\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from PIL import Image\n",
    "from miscc.config import cfg\n",
    "# from miscc.config import cfg\n",
    "from miscc.utils import mkdir_p,copy_G_params,load_params\n",
    "# from miscc.utils import build_super_images, build_super_images2\n",
    "# from miscc.utils import weights_init, load_params, copy_G_params\n",
    "\n",
    "from model import *\n",
    "from datasets import prepare_data\n",
    "from miscc.losses import sent_loss,g_loss,d_loss\n",
    "# from miscc.losses import words_loss\n",
    "# from miscc.losses import discriminator_loss, generator_loss, KL_loss\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = G_NET()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4341],\n",
       "        [4217],\n",
       "        [ 907],\n",
       "        [3066],\n",
       "        [3068],\n",
       "        [4472],\n",
       "        [  81],\n",
       "        [3066],\n",
       "        [1394],\n",
       "        [3484],\n",
       "        [1946],\n",
       "        [1394],\n",
       "        [3267],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ################# Text to image task############################ #\n",
    "class condGANTrainer(object):\n",
    "    def __init__(self, output_dir, data_loader):\n",
    "        if cfg.TRAIN.FLAG:\n",
    "            self.model_dir = os.path.join(output_dir, 'Model')\n",
    "            self.image_dir = os.path.join(output_dir, 'Image')\n",
    "            mkdir_p(self.model_dir)\n",
    "            mkdir_p(self.image_dir)\n",
    "\n",
    "#         torch.cuda.set_device(cfg.GPU_ID)\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "        self.batch_size = cfg.TRAIN.BATCH_SIZE\n",
    "        self.max_epoch = cfg.TRAIN.MAX_EPOCH\n",
    "#         self.snapshot_interval = cfg.TRAIN.SNAPSHOT_INTERVAL\n",
    "\n",
    "#         self.n_words = n_words\n",
    "#         self.ixtoword = ixtoword\n",
    "        self.data_loader = data_loader\n",
    "        self.num_batches = len(self.data_loader)\n",
    "\n",
    "    def build_models(self):\n",
    "        # ###################encoders######################################## #\n",
    "        \n",
    "        image_encoder = IMAGE_ENCODER()\n",
    "        print('Load image encoder incept v3')\n",
    "\n",
    "        text_encoder = BERT_EMBEDDING()\n",
    "        for p in text_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        print('Load text encoder bert')\n",
    "        text_encoder.eval()\n",
    "\n",
    "        # #######################generator and discriminators############## #\n",
    "        epoch = 0\n",
    "        netG = G_NET()\n",
    "        netD = D_NET()\n",
    "        if cfg.CUDA:\n",
    "            text_encoder = text_encoder.cuda()\n",
    "            image_encoder = image_encoder.cuda()\n",
    "            netG.cuda()\n",
    "            netD.cuda()\n",
    "        # ########################################################### #\n",
    "        \n",
    "\n",
    "        return [text_encoder, image_encoder, netG, netD, epoch]\n",
    "        # ########################################################### #\n",
    "        \n",
    "    def define_optimizers(self,netG, netD):\n",
    "        optimizersD = optim.Adam(netD.parameters(),\n",
    "                          lr=cfg.TRAIN.DISCRIMINATOR_LR,\n",
    "                          betas=(0.5, 0.999))\n",
    "        optimizerG = optim.Adam(netG.parameters(),\n",
    "                                lr=cfg.TRAIN.GENERATOR_LR,\n",
    "                                betas=(0.5, 0.999))\n",
    "        return optimizerG, optimizersD\n",
    "\n",
    "    def prepare_labels(self):\n",
    "        batch_size = self.batch_size\n",
    "        real_labels = Variable(torch.FloatTensor(batch_size).fill_(1))\n",
    "        fake_labels = Variable(torch.FloatTensor(batch_size).fill_(0))\n",
    "        match_labels = Variable(torch.LongTensor(range(batch_size)))\n",
    "        if cfg.CUDA:\n",
    "            real_labels = real_labels.cuda()\n",
    "            fake_labels = fake_labels.cuda()\n",
    "            match_labels = match_labels.cuda()\n",
    "\n",
    "        return real_labels, fake_labels, match_labels\n",
    "\n",
    "    def save_model(self, netG, avg_param_G, netD, epoch):\n",
    "        backup_para = copy_G_params(netG)\n",
    "        load_params(netG, avg_param_G)\n",
    "        torch.save(netG.state_dict(),\n",
    "            '%s/netG_epoch_%d.pth' % (self.model_dir, epoch))\n",
    "        load_params(netG, backup_para)\n",
    "        torch.save(netD.state_dict(),\n",
    "            '%s/netD%d.pth' % (self.model_dir, i))\n",
    "        print('Save G/Ds models.')\n",
    "\n",
    "#     def set_requires_grad_value(self, models_list, brequires):\n",
    "#         for i in range(len(models_list)):\n",
    "#             for p in models_list[i].parameters():\n",
    "#                 p.requires_grad = brequires\n",
    "\n",
    "#     def save_img_results(self, netG, noise, sent_emb, words_embs, mask,\n",
    "#                          image_encoder, captions, cap_lens,\n",
    "#                          gen_iterations, name='current'):\n",
    "#         # Save images\n",
    "#         fake_imgs, attention_maps, _, _ = netG(noise, sent_emb, words_embs, mask)\n",
    "#         for i in range(len(attention_maps)):\n",
    "#             if len(fake_imgs) > 1:\n",
    "#                 img = fake_imgs[i + 1].detach().cpu()\n",
    "#                 lr_img = fake_imgs[i].detach().cpu()\n",
    "#             else:\n",
    "#                 img = fake_imgs[0].detach().cpu()\n",
    "#                 lr_img = None\n",
    "#             attn_maps = attention_maps[i]\n",
    "#             att_sze = attn_maps.size(2)\n",
    "#             img_set, _ = \\\n",
    "#                 build_super_images(img, captions, self.ixtoword,\n",
    "#                                    attn_maps, att_sze, lr_imgs=lr_img)\n",
    "#             if img_set is not None:\n",
    "#                 im = Image.fromarray(img_set)\n",
    "#                 fullpath = '%s/G_%s_%d_%d.png'\\\n",
    "#                     % (self.image_dir, name, gen_iterations, i)\n",
    "#                 im.save(fullpath)\n",
    "\n",
    "#         # for i in range(len(netsD)):\n",
    "#         i = -1\n",
    "#         img = fake_imgs[i].detach()\n",
    "#         region_features, _ = image_encoder(img)\n",
    "#         att_sze = region_features.size(2)\n",
    "#         _, _, att_maps = words_loss(region_features.detach(),\n",
    "#                                     words_embs.detach(),\n",
    "#                                     None, cap_lens,\n",
    "#                                     None, self.batch_size)\n",
    "#         img_set, _ = \\\n",
    "#             build_super_images(fake_imgs[i].detach().cpu(),\n",
    "#                                captions, self.ixtoword, att_maps, att_sze)\n",
    "#         if img_set is not None:\n",
    "#             im = Image.fromarray(img_set)\n",
    "#             fullpath = '%s/D_%s_%d.png'\\\n",
    "#                 % (self.image_dir, name, gen_iterations)\n",
    "#             im.save(fullpath)\n",
    "\n",
    "    def train(self):\n",
    "        text_encoder, image_encoder, netG, netD, start_epoch = self.build_models()\n",
    "#         avg_param_G = copy_G_params(netG)\n",
    "        optimizerG, optimizersD = self.define_optimizers(netG, netD)\n",
    "        print(\"Load optimizers\")\n",
    "        real_labels, fake_labels, match_labels = self.prepare_labels()\n",
    "\n",
    "        batch_size = self.batch_size\n",
    "#         nz = cfg.GAN.Z_DIM\n",
    "#         noise = Variable(torch.FloatTensor(batch_size, nz))\n",
    "#         fixed_noise = Variable(torch.FloatTensor(batch_size, nz).normal_(0, 1))\n",
    "        z_code = torch.rand(batch_size, cfg.Z_DIM)\n",
    "        \n",
    "        if cfg.CUDA:\n",
    "            z_code.cuda()\n",
    "\n",
    "        gen_iterations = 0\n",
    "        # gen_iterations = start_epoch * self.num_batches\n",
    "        for epoch in range(start_epoch, self.max_epoch):\n",
    "            start_t = time.time()\n",
    "\n",
    "            data_iter = iter(self.data_loader)\n",
    "            step = 0\n",
    "#             while step < self.num_batches:\n",
    "            while step < 3:\n",
    "                # reset requires_grad to be trainable for all Ds\n",
    "                # self.set_requires_grad_value(netsD, True)\n",
    "\n",
    "                ######################################################\n",
    "                # (1) Prepare training data and Compute text embeddings\n",
    "                ######################################################\n",
    "                data = data_iter.next()\n",
    "#                 imgs, captions, cap_lens, class_ids, keys = prepare_data(data)\n",
    "                real_imgs, text, cap_lens, _, _ = prepare_data(data)\n",
    "                real_imgs = real_imgs[-1]\n",
    "                #######################################################\n",
    "                # (2) Generate fake images\n",
    "                ######################################################\n",
    "                \n",
    "                fake_imgs, words_embs, sent_emb = netG(text,z_code)\n",
    "                words_embs, sent_emb = words_embs.detach(), sent_emb.detach()\n",
    "                #######################################################\n",
    "                # (3) Update D network\n",
    "                ######################################################\n",
    "                errD = 0\n",
    "                D_logs = ''\n",
    "                netD.zero_grad()\n",
    "                errD = d_loss(netD, real_imgs, fake_imgs,sent_emb, real_labels, fake_labels)\n",
    "                # backward and update parameters\n",
    "                errD.backward()\n",
    "                optimizersD.step()\n",
    "                D_logs += 'errD: %.2f ' % (errD.data)\n",
    "\n",
    "                #######################################################\n",
    "                # (4) Update G network: maximize log(D(G(z)))\n",
    "                ######################################################\n",
    "                # compute total loss for training G\n",
    "                step += 1\n",
    "                gen_iterations += 1\n",
    "                errG = 0\n",
    "                G_logs = ''\n",
    "                # do not need to compute gradient for Ds\n",
    "                # self.set_requires_grad_value(netsD, False)\n",
    "                netG.zero_grad()\n",
    "                errG = g_loss(netD, image_encoder, fake_imgs, real_labels, sent_emb, match_labels)\n",
    "                # backward and update parameters\n",
    "                errG.backward()\n",
    "                optimizerG.step()\n",
    "                G_logs += 'errG: %.2f ' % (errG.data)\n",
    "\n",
    "                if gen_iterations % 100 == 0:\n",
    "                    print(D_logs + '\\n' + G_logs)\n",
    "                # save images\n",
    "#                 if gen_iterations % 1000 == 0:\n",
    "#                     backup_para = copy_G_params(netG)\n",
    "#                     load_params(netG, avg_param_G)\n",
    "#                     self.save_img_results(netG, fixed_noise, sent_emb,\n",
    "#                                           words_embs, mask, image_encoder,\n",
    "#                                           captions, cap_lens, epoch, name='average')\n",
    "#                     load_params(netG, backup_para)\n",
    "                    #\n",
    "                    # self.save_img_results(netG, fixed_noise, sent_emb,\n",
    "                    #                       words_embs, mask, image_encoder,\n",
    "                    #                       captions, cap_lens,\n",
    "                    #                       epoch, name='current')\n",
    "            end_t = time.time()\n",
    "\n",
    "            print('''[%d/%d][%d]\n",
    "                  Loss_D: %.2f Loss_G: %.2f Time: %.2fs'''\n",
    "                  % (epoch, self.max_epoch, self.num_batches,\n",
    "                     errD.item(), errG.item(),\n",
    "                     end_t - start_t))\n",
    "\n",
    "            if epoch % cfg.TRAIN.SNAPSHOT_INTERVAL == 0 and epoch != 0:  # and :\n",
    "                self.save_model(netG, avg_param_G, netsD, epoch)\n",
    "\n",
    "        self.save_model(netG, avg_param_G, netD, self.max_epoch)\n",
    "\n",
    "    def save_singleimages(self, images, filenames, save_dir,\n",
    "                          split_dir, sentenceID=0):\n",
    "        for i in range(images.size(0)):\n",
    "            s_tmp = '%s/single_samples/%s/%s' %\\\n",
    "                (save_dir, split_dir, filenames[i])\n",
    "            folder = s_tmp[:s_tmp.rfind('/')]\n",
    "            if not os.path.isdir(folder):\n",
    "                print('Make a new folder: ', folder)\n",
    "                mkdir_p(folder)\n",
    "\n",
    "            fullpath = '%s_%d.jpg' % (s_tmp, sentenceID)\n",
    "            # range from [-1, 1] to [0, 1]\n",
    "            # img = (images[i] + 1.0) / 2\n",
    "            img = images[i].add(1).div(2).mul(255).clamp(0, 255).byte()\n",
    "            # range from [0, 1] to [0, 255]\n",
    "            ndarr = img.permute(1, 2, 0).data.cpu().numpy()\n",
    "            im = Image.fromarray(ndarr)\n",
    "            im.save(fullpath)\n",
    "\n",
    "#     def sampling(self, split_dir):\n",
    "#         if cfg.TRAIN.NET_G == '':\n",
    "#             print('Error: the path for morels is not found!')\n",
    "#         else:\n",
    "#             if split_dir == 'test':\n",
    "#                 split_dir = 'valid'\n",
    "#             # Build and load the generator\n",
    "#             if cfg.GAN.B_DCGAN:\n",
    "#                 netG = G_DCGAN()\n",
    "#             else:\n",
    "#                 netG = G_NET()\n",
    "#             netG.apply(weights_init)\n",
    "#             netG.cuda()\n",
    "#             netG.eval()\n",
    "#             #\n",
    "#             text_encoder = RNN_ENCODER(self.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
    "#             state_dict = \\\n",
    "#                 torch.load(cfg.TRAIN.NET_E, map_location=lambda storage, loc: storage)\n",
    "#             text_encoder.load_state_dict(state_dict)\n",
    "#             print('Load text encoder from:', cfg.TRAIN.NET_E)\n",
    "#             text_encoder = text_encoder.cuda()\n",
    "#             text_encoder.eval()\n",
    "\n",
    "#             batch_size = self.batch_size\n",
    "#             nz = cfg.GAN.Z_DIM\n",
    "#             noise = Variable(torch.FloatTensor(batch_size, nz), volatile=True)\n",
    "#             noise = noise.cuda()\n",
    "\n",
    "#             model_dir = cfg.TRAIN.NET_G\n",
    "#             state_dict = \\\n",
    "#                 torch.load(model_dir, map_location=lambda storage, loc: storage)\n",
    "#             # state_dict = torch.load(cfg.TRAIN.NET_G)\n",
    "#             netG.load_state_dict(state_dict)\n",
    "#             print('Load G from: ', model_dir)\n",
    "\n",
    "#             # the path to save generated images\n",
    "#             s_tmp = model_dir[:model_dir.rfind('.pth')]\n",
    "#             save_dir = '%s/%s' % (s_tmp, split_dir)\n",
    "#             mkdir_p(save_dir)\n",
    "\n",
    "#             cnt = 0\n",
    "\n",
    "#             for _ in range(1):  # (cfg.TEXT.CAPTIONS_PER_IMAGE):\n",
    "#                 for step, data in enumerate(self.data_loader, 0):\n",
    "#                     cnt += batch_size\n",
    "#                     if step % 100 == 0:\n",
    "#                         print('step: ', step)\n",
    "#                     # if step > 50:\n",
    "#                     #     break\n",
    "\n",
    "#                     imgs, captions, cap_lens, class_ids, keys = prepare_data(data)\n",
    "\n",
    "#                     hidden = text_encoder.init_hidden(batch_size)\n",
    "#                     # words_embs: batch_size x nef x seq_len\n",
    "#                     # sent_emb: batch_size x nef\n",
    "#                     words_embs, sent_emb = text_encoder(captions, cap_lens, hidden)\n",
    "#                     words_embs, sent_emb = words_embs.detach(), sent_emb.detach()\n",
    "#                     mask = (captions == 0)\n",
    "#                     num_words = words_embs.size(2)\n",
    "#                     if mask.size(1) > num_words:\n",
    "#                         mask = mask[:, :num_words]\n",
    "\n",
    "#                     #######################################################\n",
    "#                     # (2) Generate fake images\n",
    "#                     ######################################################\n",
    "#                     noise.data.normal_(0, 1)\n",
    "#                     fake_imgs, _, _, _ = netG(noise, sent_emb, words_embs, mask)\n",
    "#                     for j in range(batch_size):\n",
    "#                         s_tmp = '%s/single/%s' % (save_dir, keys[j])\n",
    "#                         folder = s_tmp[:s_tmp.rfind('/')]\n",
    "#                         if not os.path.isdir(folder):\n",
    "#                             print('Make a new folder: ', folder)\n",
    "#                             mkdir_p(folder)\n",
    "#                         k = -1\n",
    "#                         # for k in range(len(fake_imgs)):\n",
    "#                         im = fake_imgs[k][j].data.cpu().numpy()\n",
    "#                         # [-1, 1] --> [0, 255]\n",
    "#                         im = (im + 1.0) * 127.5\n",
    "#                         im = im.astype(np.uint8)\n",
    "#                         im = np.transpose(im, (1, 2, 0))\n",
    "#                         im = Image.fromarray(im)\n",
    "#                         fullpath = '%s_s%d.png' % (s_tmp, k)\n",
    "#                         im.save(fullpath)\n",
    "\n",
    "#     def gen_example(self, data_dic):\n",
    "#         if cfg.TRAIN.NET_G == '':\n",
    "#             print('Error: the path for morels is not found!')\n",
    "#         else:\n",
    "#             # Build and load the generator\n",
    "#             text_encoder = \\\n",
    "#                 RNN_ENCODER(self.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
    "#             state_dict = \\\n",
    "#                 torch.load(cfg.TRAIN.NET_E, map_location=lambda storage, loc: storage)\n",
    "#             text_encoder.load_state_dict(state_dict)\n",
    "#             print('Load text encoder from:', cfg.TRAIN.NET_E)\n",
    "#             text_encoder = text_encoder.cuda()\n",
    "#             text_encoder.eval()\n",
    "\n",
    "#             # the path to save generated images\n",
    "#             if cfg.GAN.B_DCGAN:\n",
    "#                 netG = G_DCGAN()\n",
    "#             else:\n",
    "#                 netG = G_NET()\n",
    "#             s_tmp = cfg.TRAIN.NET_G[:cfg.TRAIN.NET_G.rfind('.pth')]\n",
    "#             model_dir = cfg.TRAIN.NET_G\n",
    "#             state_dict = \\\n",
    "#                 torch.load(model_dir, map_location=lambda storage, loc: storage)\n",
    "#             netG.load_state_dict(state_dict)\n",
    "#             print('Load G from: ', model_dir)\n",
    "#             netG.cuda()\n",
    "#             netG.eval()\n",
    "#             for key in data_dic:\n",
    "#                 save_dir = '%s/%s' % (s_tmp, key)\n",
    "#                 mkdir_p(save_dir)\n",
    "#                 captions, cap_lens, sorted_indices = data_dic[key]\n",
    "\n",
    "#                 batch_size = captions.shape[0]\n",
    "#                 nz = cfg.GAN.Z_DIM\n",
    "#                 captions = Variable(torch.from_numpy(captions), volatile=True)\n",
    "#                 cap_lens = Variable(torch.from_numpy(cap_lens), volatile=True)\n",
    "\n",
    "#                 captions = captions.cuda()\n",
    "#                 cap_lens = cap_lens.cuda()\n",
    "#                 for i in range(1):  # 16\n",
    "#                     noise = Variable(torch.FloatTensor(batch_size, nz), volatile=True)\n",
    "#                     noise = noise.cuda()\n",
    "#                     #######################################################\n",
    "#                     # (1) Extract text embeddings\n",
    "#                     ######################################################\n",
    "#                     hidden = text_encoder.init_hidden(batch_size)\n",
    "#                     # words_embs: batch_size x nef x seq_len\n",
    "#                     # sent_emb: batch_size x nef\n",
    "#                     words_embs, sent_emb = text_encoder(captions, cap_lens, hidden)\n",
    "#                     mask = (captions == 0)\n",
    "#                     #######################################################\n",
    "#                     # (2) Generate fake images\n",
    "#                     ######################################################\n",
    "#                     noise.data.normal_(0, 1)\n",
    "#                     fake_imgs, attention_maps, _, _ = netG(noise, sent_emb, words_embs, mask)\n",
    "#                     # G attention\n",
    "#                     cap_lens_np = cap_lens.cpu().data.numpy()\n",
    "#                     for j in range(batch_size):\n",
    "#                         save_name = '%s/%d_s_%d' % (save_dir, i, sorted_indices[j])\n",
    "#                         for k in range(len(fake_imgs)):\n",
    "#                             im = fake_imgs[k][j].data.cpu().numpy()\n",
    "#                             im = (im + 1.0) * 127.5\n",
    "#                             im = im.astype(np.uint8)\n",
    "#                             # print('im', im.shape)\n",
    "#                             im = np.transpose(im, (1, 2, 0))\n",
    "#                             # print('im', im.shape)\n",
    "#                             im = Image.fromarray(im)\n",
    "#                             fullpath = '%s_g%d.png' % (save_name, k)\n",
    "#                             im.save(fullpath)\n",
    "\n",
    "#                         for k in range(len(attention_maps)):\n",
    "#                             if len(fake_imgs) > 1:\n",
    "#                                 im = fake_imgs[k + 1].detach().cpu()\n",
    "#                             else:\n",
    "#                                 im = fake_imgs[0].detach().cpu()\n",
    "#                             attn_maps = attention_maps[k]\n",
    "#                             att_sze = attn_maps.size(2)\n",
    "#                             img_set, sentences = \\\n",
    "#                                 build_super_images2(im[j].unsqueeze(0),\n",
    "#                                                     captions[j].unsqueeze(0),\n",
    "#                                                     [cap_lens_np[j]], self.ixtoword,\n",
    "#                                                     [attn_maps[j]], att_sze)\n",
    "#                             if img_set is not None:\n",
    "#                                 im = Image.fromarray(img_set)\n",
    "#                                 fullpath = '%s_a%d.png' % (save_name, k)\n",
    "#                                 im.save(fullpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.load('testcase.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_singleimages(data,z_code,netG, save_dir,\n",
    "                      split_dir,gen_iterations):\n",
    "\n",
    "    real_imgs, text, _, _, keys = prepare_data(data)\n",
    "#         real_imgs = real_imgs[-1]\n",
    "    fake_imgs, _, _ = netG(text,z_code)\n",
    "    real_imgs = real_imgs[-1]\n",
    "    for i in range(fake_imgs.size(0)):\n",
    "        s_tmp = '%s/single_samples/%s/gen%s/' %\\\n",
    "            (save_dir, split_dir, gen_iterations//1000)\n",
    "        folder = s_tmp[:s_tmp.rfind('/')]\n",
    "        if not os.path.isdir(folder):\n",
    "            print('Make a new folder: ', folder)\n",
    "            mkdir_p(folder)\n",
    "            \n",
    "        key = keys[i].replace('.','_')\n",
    "        key = key.replace('/' , '_')\n",
    "        tokens = '_'.join([tokenizer.convert_ids_to_tokens([w.item()])[0] for w in text[i]])\n",
    "        print(tokens)\n",
    "        fullpath_fake = '%s_%s_fake.jpg' % (s_tmp, tokens)\n",
    "        fullpath_real = '%s_%s_real.jpg' % (s_tmp, tokens)\n",
    "        \n",
    "\n",
    "        # range from [-1, 1] to [0, 1]\n",
    "        # img = (images[i] + 1.0) / 2\n",
    "        img = fake_imgs[i].add(1).div(2).mul(255).clamp(0, 255).byte()\n",
    "        # range from [0, 1] to [0, 255]\n",
    "        ndarr = img.permute(1, 2, 0).data.cpu().numpy()\n",
    "        im = Image.fromarray(ndarr)\n",
    "        im.save(fullpath_fake)\n",
    "        \n",
    "        img2 = real_imgs[i].add(1).div(2).mul(255).clamp(0, 255).byte()\n",
    "        # range from [0, 1] to [0, 255]\n",
    "        ndarr = img2.permute(1, 2, 0).data.cpu().numpy()\n",
    "        im2 = Image.fromarray(ndarr)\n",
    "        im2.save(fullpath_real)\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(cfg.TEXT.PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_code = torch.rand(test[1].shape[0], 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make a new folder:  ../output/single_samples/test/gen1\n",
      "colorful_white_belly_and_black_wings_with_two_wing_,_bill_and_,_black_##sus_feet_._[SEP]\n",
      "varied_colors_of_this_vibrant_bird_include_yellow_red_,_and_orange_throughout_its_head_and_body_[SEP]\n",
      "[CLS]_a_very_colorful_bird_with_a_black_head_black_and_white_wings_some_red_on_._[SEP]\n",
      "[CLS]_this_a_large_mostly_black_bird_with_on_its_neck_and_on_the_top_of_head_[SEP]\n",
      "[CLS]_a_bird_with_brown_and_white_feathers_with_a_long_tail_and_a_brown_bill_._[SEP]\n",
      "[CLS]_a_small_brown_bird_with_a_sharp_white_beak_and_large_white_talon_##s_._[SEP]_[PAD]\n",
      "[CLS]_the_multi_-_colored_bird_has_a_long_narrow_bill_and_a_black_crown_._[SEP]_[PAD]\n",
      "[CLS]_this_is_a_black_bird_with_a_white_breast_and_belly_and_brown_sides_[SEP]_[PAD]_[PAD]\n",
      "[CLS]_this_bird_is_green_and_black_in_color_,_with_a_black_beak_._[SEP]_[PAD]_[PAD]\n",
      "[CLS]_bird_has_brown_body_feathers_,_brown_breast_feathers_,_and_brown_beak_[SEP]_[PAD]_[PAD]_[PAD]\n",
      "[CLS]_this_bird_is_yellow_and_black_and_has_a_very_short_beak_._[SEP]_[PAD]_[PAD]_[PAD]\n",
      "[CLS]_this_bird_is_brown_with_red_and_has_a_very_short_beak_._[SEP]_[PAD]_[PAD]_[PAD]\n",
      "[CLS]_this_bird_is_yellow_with_grey_and_has_a_very_short_beak_._[SEP]_[PAD]_[PAD]_[PAD]\n",
      "[CLS]_this_bird_is_white_in_color_,_with_a_light_colored_beak_._[SEP]_[PAD]_[PAD]_[PAD]\n",
      "[CLS]_this_is_a_dark_bird_with_dark_feathers_a_dark_beak_._[SEP]_[PAD]_[PAD]_[PAD]_[PAD]\n",
      "[CLS]_this_bird_has_wings_that_are_brown_and_has_a_tan_belly_[SEP]_[PAD]_[PAD]_[PAD]_[PAD]\n",
      "[CLS]_this_bird_has_wings_that_are_black_and_has_a_thick_bill_[SEP]_[PAD]_[PAD]_[PAD]_[PAD]\n",
      "[CLS]_this_bird_has_black_wings_with_a_white_striped_body_._[SEP]_[PAD]_[PAD]_[PAD]_[PAD]_[PAD]\n",
      "[CLS]_a_large_white_and_gray_bird_with_a_silver_beak_._[SEP]_[PAD]_[PAD]_[PAD]_[PAD]_[PAD]\n",
      "[CLS]_a_bird_with_a_black_crown_and_a_red_belly_._[SEP]_[PAD]_[PAD]_[PAD]_[PAD]_[PAD]\n"
     ]
    }
   ],
   "source": [
    "save_singleimages(test,z_code,netG, '../output','test',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    " _, text, _, _, keys = prepare_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f2de39a8e043>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pytorch_pretrained_bert/tokenization.py\u001b[0m in \u001b[0;36mconvert_tokens_to_ids\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             logger.warning(\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [tokenizer.convert_ids_to_tokens([x.item()])[0] for x in test[1][0]]\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] a small brown bird with a sharp white beak and large white talon ##s . [SEP] [PAD]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[CLS]'],\n",
       " ['a'],\n",
       " ['small'],\n",
       " ['brown'],\n",
       " ['bird'],\n",
       " ['with'],\n",
       " ['a'],\n",
       " ['sharp'],\n",
       " ['white'],\n",
       " ['beak'],\n",
       " ['and'],\n",
       " ['large'],\n",
       " ['white'],\n",
       " ['talon'],\n",
       " ['##s'],\n",
       " ['.'],\n",
       " ['[SEP]'],\n",
       " ['[PAD]']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = [self.tokenizer.convert_tokens_to_ids(\n",
    "                self.tokenizer.tokenize('[CLS] ' + sent + ' [SEP]')) for sent in train_captions]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
