{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "\n",
    "__C = edict()\n",
    "config = __C\n",
    "\n",
    "\n",
    "__C.CUDA = False\n",
    "__C.E_DIM = 768          # Word embedding dimension\n",
    "__C.C_DIM = 256          # Condition c_code dimension\n",
    "__C.Z_DIM = 256          # Random z_code dimension\n",
    "__C.W_DIM = 512          # Latent w_code dimension\n",
    "__C.A_DIM = 512          # Attention a_code dimension\n",
    "__C.RESOLUTION = 512    # Target image's resolution\n",
    "\n",
    "## TEXT\n",
    "__C.TEXT = edict()\n",
    "__C.TEXT.PRETRAINED_MODEL = 'bert-base-uncased'\n",
    "__C.TEXT.MAX_LENGTH = 18\n",
    "\n",
    "\n",
    "## MAPPING\n",
    "__C.M = edict()\n",
    "__C.M.LAYERS = 8\n",
    "\n",
    "## GENERATOR\n",
    "__C.G = edict()\n",
    "\n",
    "\n",
    "\n",
    "## DISCRIMINATOR\n",
    "__C.D = edict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT_EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_EMBEDDING(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT_EMBEDDING, self).__init__()\n",
    "        self.max_length = config.TEXT.MAX_LENGTH\n",
    "        self.pretrained_model = config.TEXT.PRETRAINED_MODEL\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.pretrained_model)\n",
    "        self.model = BertModel.from_pretrained(self.pretrained_model)\n",
    "    \n",
    "    #####TODO#####\n",
    "    def forward(self, text):\n",
    "        '''\n",
    "        Inputs:\n",
    "            text: list of raw texts, [batch_size]\n",
    "            \n",
    "        Outputs:\n",
    "            encoded_layers: [batch_size, max_length, 768], word embedding\n",
    "            pooled_layer: [batch_size, 768], \n",
    "        '''\n",
    "        indexed_tokens = [self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize('[CLS] ' + sent + ' [SEP]')) for sent in text]\n",
    "        # bucket_len = max([len(sent) for sent in indexed_tokens])\n",
    "        input_ids, segment_ids, mask_ids = zip(*(self._make_inputs(sent) for sent in indexed_tokens))\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        segment_ids = torch.tensor(segment_ids)\n",
    "        mask_ids = torch.tensor(mask_ids)\n",
    "        \n",
    "        encoded_layers, pooled_layer = self.model(input_ids, segment_ids, mask_ids, output_all_encoded_layers=False)\n",
    "        encoded_layers = torch.transpose(encoded_layers, 1, 2)\n",
    "        \n",
    "        return encoded_layers, pooled_layer\n",
    "    \n",
    "    def _make_inputs(self, indexed_tokens):\n",
    "        '''Convert tokenized text to three inputs as Bert required\n",
    "        \n",
    "        indexed_tokens: list of tokenized test, shape[batch_size, none]\n",
    "        '''\n",
    "        \n",
    "        l = len(indexed_tokens)\n",
    "        # max_len = min(bucket_len, self.max_length)\n",
    "        if l >= self.max_length:\n",
    "            input_ids = indexed_tokens[:self.max_length]\n",
    "            segment_ids = [0] * self.max_length\n",
    "            mask_ids = [1] * self.max_length\n",
    "        else:\n",
    "            input_ids = indexed_tokens + [0]*(self.max_length - l) # 0 is '[PAD]' in Bert\n",
    "            segment_ids = [0] * self.max_length\n",
    "            mask_ids = [1] * l + [0] * (self.max_length - l)\n",
    "        \n",
    "        return input_ids, segment_ids, mask_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 6048995.05B/s]\n",
      "100%|██████████| 407873900/407873900 [00:22<00:00, 18409625.54B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 768, 18])\n",
      "torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "## test\n",
    "text = ['It is a big dog.', \"I'm a boy\", \"It's the end. Don't continue.\"]\n",
    "bert_embedding = BERT_EMBEDDING()\n",
    "encoded_layers, pooled_layer = bert_embedding(text)\n",
    "print(encoded_layers.shape)\n",
    "print(pooled_layer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CA_NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GLU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        nc = x.size(1)\n",
    "        assert nc % 2 == 0, 'channels dont divide 2!'\n",
    "        nc = int(nc/2)\n",
    "        return x[:, :nc] * torch.sigmoid(x[:, nc:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CA_NET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CA_NET, self).__init__()\n",
    "        self.e_dim = config.E_DIM\n",
    "        self.c_dim = config.C_DIM\n",
    "        self.fc = nn.Linear(self.e_dim, self.c_dim * 4, bias=True)\n",
    "        self.relu = GLU()\n",
    "\n",
    "    def encode(self, text_embedding):\n",
    "        x = self.relu(self.fc(text_embedding))\n",
    "        mu = x[:, :self.c_dim]\n",
    "        log_var = x[:, self.c_dim:]\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparametrize(self, mu, log_var):\n",
    "        std = log_var.mul(0.5).exp_()\n",
    "        if config.CUDA:\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def forward(self, text_embedding):\n",
    "        '''\n",
    "        Inputs:\n",
    "            text_embedding: [batch_size, config.E_DIM=768]\n",
    "            \n",
    "        Outputs:\n",
    "            c_code: [batch_size, config.C_DIM], reparametrized text embedding\n",
    "            mu: [batch_size, config.C_DIM], mean\n",
    "            log_var: [batch_size, config.C_DIM], logVariance\n",
    "        '''\n",
    "        \n",
    "        mu, log_var = self.encode(text_embedding)\n",
    "        c_code = self.reparametrize(mu, log_var)\n",
    "        return c_code, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256])\n"
     ]
    }
   ],
   "source": [
    "## test\n",
    "ca_net = CA_NET()\n",
    "t_code, mu, log_var = ca_net(pooled_layer)\n",
    "print(t_code.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G_MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 gain=2**(0.5),\n",
    "                 use_wscale=False,\n",
    "                 lrmul=1.0,\n",
    "                 bias=True):\n",
    "        \"\"\"\n",
    "            The complete conversion of Dense/FC/Linear Layer of original Tensorflow version.\n",
    "        \"\"\"\n",
    "        super(FC, self).__init__()\n",
    "        he_std = gain * in_channels ** (-0.5)  # He init\n",
    "        if use_wscale:\n",
    "            init_std = 1.0 / lrmul\n",
    "            self.w_lrmul = he_std * lrmul\n",
    "        else:\n",
    "            init_std = he_std / lrmul\n",
    "            self.w_lrmul = lrmul\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(out_channels, in_channels) * init_std)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_channels))\n",
    "            self.b_lrmul = lrmul\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.bias is not None:\n",
    "            out = F.linear(x, self.weight * self.w_lrmul, self.bias * self.b_lrmul)\n",
    "        else:\n",
    "            out = F.linear(x, self.weight * self.w_lrmul)\n",
    "        out = F.leaky_relu(out, 0.2, inplace=True)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class Pixel_Norm(nn.Module):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super(Pixel_Norm, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        tmp  = torch.mul(x, x) # or x ** 2\n",
    "        tmp1 = torch.rsqrt(torch.mean(tmp, dim=1, keepdim=True) + self.epsilon)\n",
    "\n",
    "        return x * tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G_MAPPING(nn.Module):\n",
    "    def __init__(self,\n",
    "                 normalize_latents=True, # Normalize latent vector?\n",
    "                 use_wscale=True,        # Enable equalized learning rate?\n",
    "                 lrmul=0.01,             # Learning rate multiplier for the mapping layers\n",
    "                 gain=2**(0.5),          # Original gain in tensorflow.\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super(G_MAPPING, self).__init__()\n",
    "        self.num_layers = config.M.LAYERS\n",
    "        self.c_dim = config.C_DIM\n",
    "        self.z_dim = config.Z_DIM\n",
    "        self.w_dim = config.W_DIM\n",
    "        self.concat_dim = self.c_dim + self.z_dim\n",
    "        self.normalize_latents = normalize_latents\n",
    "        if normalize_latents:\n",
    "            self.pixel_norm = Pixel_Norm()\n",
    "        else:\n",
    "            self.pixel_norm = None\n",
    "        \n",
    "        self.mapping = nn.ModuleList()\n",
    "        for idx in range(self.num_layers):\n",
    "            if idx == 0:\n",
    "                self.mapping.append(FC(self.concat_dim, self.w_dim, gain, lrmul, use_wscale))\n",
    "            else:\n",
    "                self.mapping.append(FC(self.w_dim, self.w_dim, gain, lrmul, use_wscale))\n",
    "\n",
    "    def forward(self, c_code, z_code):\n",
    "        '''\n",
    "        Inputs:\n",
    "            c_code: [batch_size, config.M.CONDITION_DIM+config.M.LATENT_DIM], text after CA_NET\n",
    "            z_code: [batch_size, config.M.MAPPING_DIM], noise(Z) generated from some distribution\n",
    "            \n",
    "        Outputs:\n",
    "            w_code: [batch_size, config.M.MAPPING_DIM], latent(W)\n",
    "        '''\n",
    "        if self.normalize_latents:\n",
    "            z_code = self.pixel_norm(z_code)\n",
    "        w_code = torch.cat((c_code, z_code), dim=1)\n",
    "        for fc in self.mapping:\n",
    "            w_code = fc(w_code)\n",
    "        return w_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512])\n"
     ]
    }
   ],
   "source": [
    "## test\n",
    "mapping = G_MAPPING()\n",
    "z_code = torch.rand(3, config.Z_DIM)\n",
    "w_code = mapping(t_code, z_code)\n",
    "print(w_code.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1x1(in_planes, out_planes):\n",
    "    \"1x1 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
    "                     padding=0, bias=False)\n",
    "\n",
    "class GlobalAttentionGeneral(nn.Module):\n",
    "    def __init__(self, channels, res):\n",
    "        super(GlobalAttentionGeneral, self).__init__()\n",
    "        self.cdf = config.E_DIM\n",
    "        self.idf = channels\n",
    "        self.ih = res\n",
    "        self.iw = res\n",
    "        self.a_dim = config.A_DIM\n",
    "        self.conv_context = conv1x1(self.cdf, self.idf)\n",
    "        self.sm = nn.Softmax(dim = 1)\n",
    "        self.mask = None\n",
    "        self.att_fc = nn.Linear(self.idf*self.ih*self.iw, self.a_dim, bias=True)\n",
    "\n",
    "    def applyMask(self, mask):\n",
    "        self.mask = mask  # batch x sourceL\n",
    "\n",
    "    def forward(self, inputs, context):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            inputs: batch x idf x ih x iw (queryL=ihxiw)   image(h) idf = deepth\n",
    "            context: batch x cdf x sourceL                 word embedding sequence, cdf = 768, sourceL = 18\n",
    "        \"\"\"\n",
    "        ih, iw = inputs.size(2), inputs.size(3)\n",
    "        queryL = ih * iw\n",
    "        batch_size, sourceL = context.size(0), context.size(2)\n",
    "\n",
    "\n",
    "        target = inputs.view(batch_size, -1, queryL)\n",
    "        # --> batch x queryL x idf\n",
    "        targetT = torch.transpose(target, 1, 2).contiguous()\n",
    "\n",
    "        # batch x cdf x sourceL --> batch x cdf x sourceL x 1\n",
    "        sourceT = context.unsqueeze(3)\n",
    "\n",
    "        # --> batch x idf x sourceL\n",
    "        sourceT = self.conv_context(sourceT).squeeze(3)\n",
    "\n",
    "        # Get attention\n",
    "        #           h                       e'\n",
    "        # (batch x queryL x idf)(batch x idf x sourceL)\n",
    "        # -->batch x queryL x sourceL\n",
    "        attn = torch.bmm(targetT, sourceT)\n",
    "\n",
    "        # --> batch*queryL x sourceL\n",
    "        attn = attn.view(batch_size*queryL, sourceL)\n",
    "\n",
    "        if self.mask is not None:\n",
    "            # batch_size x sourceL --> batch_size*queryL x sourceL\n",
    "            mask = self.mask.repeat(queryL, 1)\n",
    "            attn.data.masked_fill_(mask.data, -float('inf'))\n",
    "\n",
    "        attn = self.sm(attn)  # Eq. (2)\n",
    "        # --> batch x queryL x sourceL\n",
    "        attn = attn.view(batch_size, queryL, sourceL)\n",
    "        # --> batch x sourceL x queryL\n",
    "        attn = torch.transpose(attn, 1, 2).contiguous()\n",
    "\n",
    "        # (batch x idf x sourceL)(batch x sourceL x queryL)\n",
    "        # --> batch x idf x queryL\n",
    "        weightedContext = torch.bmm(sourceT, attn)\n",
    "        # --> batch x (idf * queryL)\n",
    "        weightedContext = weightedContext.view(batch_size, -1)\n",
    "        weightedContext = self.att_fc(weightedContext)\n",
    "\n",
    "        return weightedContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512])\n"
     ]
    }
   ],
   "source": [
    "## test\n",
    "image = torch.rand(3, 512, 16, 16)\n",
    "attn = GlobalAttentionGeneral(512, 16)\n",
    "a_code = attn(image, encoded_layers)\n",
    "print(a_code.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Apply_Style(nn.Module):\n",
    "    def __init__(self,\n",
    "                 channels,      # Channels\n",
    "                 res,           # Resolution\n",
    "                 use_attn = True,\n",
    "                 use_wscale = True,\n",
    "                 **kwargs\n",
    "                ):\n",
    "        super(Apply_Style, self).__init__()\n",
    "        self.w_dim = config.W_DIM\n",
    "        self.a_dim = config.A_DIM\n",
    "        self.channels = channels\n",
    "        self.res = res\n",
    "        self.use_attn = use_attn\n",
    "        if self.use_attn:\n",
    "            self.attn = GlobalAttentionGeneral(channels, res)\n",
    "            self.fc = FC(self.w_dim + self.a_dim,\n",
    "                         self.channels*2,\n",
    "                         gain=1.0,\n",
    "                         use_wscale = use_wscale\n",
    "                        )\n",
    "        else:\n",
    "            self.fc = FC(self.w_dim,\n",
    "                         self.channels*2,\n",
    "                         gain=1.0,\n",
    "                         use_wscale = use_wscale\n",
    "                        )\n",
    "        \n",
    "    def forward(self, x, w_code, word_embedding):\n",
    "        '''\n",
    "        Inputs:\n",
    "            x: [batch_size, num_features, height, width], outputs of last synthesis process\n",
    "            w_code: [batch_size, config.W_DIM], latent(W)\n",
    "            word_embedding: [batch_size, max_length, 768], word embedding\n",
    "        Outputs:\n",
    "            x: [batch_size, num_features, height, width]\n",
    "        '''\n",
    "        if self.use_attn:\n",
    "            attn_code = self.attn(x, word_embedding)\n",
    "            style_code = torch.cat((attn_code, w_code), dim=1)\n",
    "            style_code = self.fc(style_code) # [batch_size, n_channels*2]\n",
    "        else:\n",
    "            style_code = self.fc(w_code)     # [batch_size, n_channels*2]\n",
    "\n",
    "        shape = [-1, 2, x.size(1), 1, 1]\n",
    "        style_code = style_code.view(shape) \n",
    "        x = x * (style_code[:, 0] + 1.) + style_code[:, 1]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "## test\n",
    "apply_style = Apply_Style(512, 16)\n",
    "style_code = apply_style(image, w_code, encoded_layers)\n",
    "print(style_code.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Apply_Noise(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        '''\n",
    "        '''\n",
    "        super(Apply_Noise, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(channels))\n",
    "\n",
    "    def forward(self, x, noise):\n",
    "        if noise is None:\n",
    "            noise = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device, dtype=x.dtype)\n",
    "        return x + self.weight.view(1, -1, 1, 1) * noise.to(x.device)\n",
    "\n",
    "\n",
    "class Blur2d(nn.Module):\n",
    "    def __init__(self, f=[1,2,1], normalize=True, flip=False, stride=1):\n",
    "        '''\n",
    "        '''\n",
    "        super(Blur2d, self).__init__()\n",
    "        assert isinstance(f, list) or f is None, \"kernel f must be an instance of python built_in type list!\"\n",
    "\n",
    "        if f is not None:\n",
    "            f = torch.tensor(f, dtype=torch.float32)\n",
    "            f = f[:, None] * f[None, :]\n",
    "            f = f[None, None]\n",
    "            if normalize:\n",
    "                f = f / f.sum()\n",
    "            if flip:\n",
    "                f = torch.flip(f, [2, 3])\n",
    "            self.f = f\n",
    "        else:\n",
    "            self.f = None\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.f is not None:\n",
    "            # expand kernel channels\n",
    "            kernel = self.f.expand(x.size(1), -1, -1, -1).to(x.device)\n",
    "            x = nn.conv2d(\n",
    "                x,\n",
    "                kernel,\n",
    "                stride=self.stride,\n",
    "                padding=int((self.f.size(2)-1)/2),\n",
    "                groups=x.size(1)\n",
    "            )\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class Conv2d(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_channels,\n",
    "                 output_channels,\n",
    "                 kernel_size,\n",
    "                 gain=2 ** (0.5),\n",
    "                 use_wscale=False,\n",
    "                 lrmul=1,\n",
    "                 bias=True):\n",
    "        '''\n",
    "        '''\n",
    "        super(Conv2d, self).__init__()\n",
    "        he_std = gain * (input_channels * kernel_size ** 2) ** (-0.5)  # He init\n",
    "        self.kernel_size = kernel_size\n",
    "        if use_wscale:\n",
    "            init_std = 1.0 / lrmul\n",
    "            self.w_lrmul = he_std * lrmul\n",
    "        else:\n",
    "            init_std = he_std / lrmul\n",
    "            self.w_lrmul = lrmul\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(output_channels, input_channels, kernel_size, kernel_size) * init_std)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(output_channels))\n",
    "            self.b_lrmul = lrmul\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.bias is not None:\n",
    "            return F.conv2d(x, self.weight * self.w_lrmul, self.bias * self.b_lrmul, padding=self.kernel_size // 2)\n",
    "        else:\n",
    "            return F.conv2d(x, self.weight * self.w_lrmul, padding=self.kernel_size // 2)\n",
    "\n",
    "class Upscale2d(nn.Module):\n",
    "    def __init__(self, factor=2, gain=1):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        super(Upscale2d, self).__init__()\n",
    "        self.gain = gain\n",
    "        self.factor = factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.gain != 1:\n",
    "            x = x * self.gain\n",
    "        if self.factor > 1:\n",
    "            shape = x.shape\n",
    "            x = x.view(shape[0], shape[1], shape[2], 1, shape[3], 1).expand(-1, -1, -1, self.factor, -1, self.factor)\n",
    "            x = x.contiguous().view(shape[0], shape[1], self.factor * shape[2], self.factor * shape[3])\n",
    "        return x\n",
    "\n",
    "class Instance_Norm(nn.Module):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        \"\"\" \n",
    "        \"\"\"\n",
    "        super(Instance_Norm, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        x   = x - torch.mean(x, (2, 3), True)\n",
    "        tmp = torch.mul(x, x) # or x ** 2\n",
    "        tmp = torch.rsqrt(torch.mean(tmp, (2, 3), True) + self.epsilon)\n",
    "        return x * tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G_BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Epilogue(nn.Module):\n",
    "    def __init__(self,\n",
    "                 channels,\n",
    "                 res,\n",
    "                 use_attn = True,\n",
    "                 use_wscale = True,\n",
    "                 use_noise = True,\n",
    "                 use_pixel_norm = False,\n",
    "                 use_instance_norm = True,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super(Layer_Epilogue, self).__init__()\n",
    "        self.act = nn.LeakyReLU(negative_slope=0.2)\n",
    "\n",
    "        if use_noise:\n",
    "            self.noise = Apply_Noise(channels)\n",
    "        \n",
    "        if use_pixel_norm:\n",
    "            self.pixel_norm = Pixel_Norm()\n",
    "        else:\n",
    "            self.pixel_norm = None\n",
    "\n",
    "        if use_instance_norm:\n",
    "            self.instance_norm = Instance_Norm()\n",
    "        else:\n",
    "            self.instance_norm = None\n",
    "\n",
    "        self.style_mod = Apply_Style(channels, res, **kwargs)\n",
    "\n",
    "    def forward(self, x, w_code, word_embedding, noise=None):\n",
    "        '''\n",
    "        Inputs:\n",
    "            x: [batch_size, channels, res, res]\n",
    "            w_code: [batch_size, config.M.MAPPING_DIM], latent(W)\n",
    "            word_embedding: [batch_size, max_length, 768], word embedding\n",
    "            noise:\n",
    "        Outputs:\n",
    "            x: [batch_size, num_features, height, width]\n",
    "        '''\n",
    "        x = self.noise(x, noise)\n",
    "        x = self.act(x)\n",
    "        if self.pixel_norm is not None:\n",
    "            x = self.pixel_norm(x)\n",
    "        if self.instance_norm is not None:\n",
    "            x = self.instance_norm(x)\n",
    "        x = self.style_mod(x, w_code, word_embedding)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "## test\n",
    "layer_epilogue = Layer_Epilogue(512, 16)\n",
    "layer_out = layer_epilogue(image, w_code, encoded_layers)\n",
    "print(layer_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G_BLOCK(nn.Module):\n",
    "    def __init__(self,\n",
    "                 log2_res,           # Current Resolution,  3.4...log_2(resolution)\n",
    "                 use_attn = True,\n",
    "                 use_wscale = True,\n",
    "                 use_noise = True,\n",
    "                 use_pixel_norm = False,\n",
    "                 use_instance_norm = True,\n",
    "                 f=None,             # (Huge overload, if you dont have enough resouces, please pass it as `f = None`)Low-pass filter to apply when resampling activations. None = no filtering.\n",
    "                 factor=2,           # upsample factor.\n",
    "                 fmap_base=8192,     # Overall multiplier for the number of feature maps.\n",
    "                 fmap_decay=1.0,     # log2 feature map reduction when doubling the resolution.\n",
    "                 fmap_max=512,       # Maximum number of feature maps in any layer.\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super(G_BLOCK, self).__init__()\n",
    "        self.nf = lambda res: min(int(fmap_base / (2.0 ** (res * fmap_decay))), fmap_max)\n",
    "        \n",
    "\n",
    "        # res\n",
    "        self.log2_res = log2_res\n",
    "        \n",
    "        self.channel = self.nf(self.log2_res)\n",
    "        \n",
    "        # blur2d\n",
    "        self.blur = Blur2d(f)\n",
    "\n",
    "        # noise\n",
    "        # self.noise = noise\n",
    "\n",
    "        if self.nf(self.log2_res-1) == self.channel:\n",
    "            # upsample method 1, \n",
    "            self.up_sample = Upscale2d(factor)\n",
    "        else:\n",
    "            # upsample method 2\n",
    "            self.up_sample = nn.ConvTranspose2d(self.nf(self.log2_res-1), self.channel, 4, stride=2, padding=1)\n",
    "\n",
    "        # A Composition of LayerEpilogue and Conv2d.\n",
    "        self.adaIn1 = Layer_Epilogue(self.channel,\n",
    "                                     2 ** self.log2_res,\n",
    "                                     use_attn = use_attn,\n",
    "                                     use_wscale = use_wscale,\n",
    "                                     use_noise = use_noise,\n",
    "                                     use_pixel_norm = use_pixel_norm,\n",
    "                                     use_instance_norm = use_instance_norm\n",
    "                                    )\n",
    "        self.conv1  = Conv2d(self.channel, self.channel,\n",
    "                             kernel_size=3, use_wscale=use_wscale)\n",
    "        self.adaIn2 = Layer_Epilogue(self.channel,\n",
    "                                     2 ** self.log2_res,\n",
    "                                     use_attn = use_attn,\n",
    "                                     use_wscale = use_wscale,\n",
    "                                     use_noise = use_noise,\n",
    "                                     use_pixel_norm = use_pixel_norm,\n",
    "                                     use_instance_norm = use_instance_norm\n",
    "                                    )\n",
    "\n",
    "    def forward(self, x, w_code, word_embedding, noise=None):\n",
    "        '''\n",
    "        Inputs:\n",
    "            x: [batch_size, channels, res, res]\n",
    "            w_code: [batch_size, config.M.MAPPING_DIM], latent(W)\n",
    "            word_embedding: [batch_size, max_length, 768], word embedding\n",
    "            noise:\n",
    "        Outputs:\n",
    "            x: [batch_size, num_features, height, width]\n",
    "        '''\n",
    "        x = self.up_sample(x)\n",
    "        x = self.blur(x)\n",
    "        x = self.adaIn1(x, w_code, word_embedding, noise)\n",
    "        x = self.conv1(x)\n",
    "        x = self.adaIn2(x, w_code, word_embedding, noise)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "## test\n",
    "g_block = G_BLOCK(5)\n",
    "block_out = g_block(image, w_code, encoded_layers)\n",
    "print(block_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G_NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G_NET(nn.Module):\n",
    "    def __init__(self,\n",
    "                 fmap_base = 8192,         # Overall multiplier for the number of feature maps\n",
    "                 out_channels = 3,         # Number of output image colors\n",
    "                 structure = 'fixed',      # 'fixed' = no progressive growing, 'linear' = human-readable, 'recursive' = efficient\n",
    "                 fmap_max = 512,           # Maximum inumber of feature maps in any layer\n",
    "                 fmap_decay = 1.0,         # log2 feature map reduction when doubling the resolution\n",
    "                 f=None,                   # (Huge overload, if you dont have enough resouces, please pass it as `f = None`)Low-pass filter to apply when resampling activations. None = no filtering.\n",
    "                 use_attn = True,          # Enable attention style?\n",
    "                 use_pixel_norm = False,   # Enable pixelwise feature vector normalization?\n",
    "                 use_instance_norm = True, # Enable instance normalization?\n",
    "                 use_wscale = True,        # Enable equalized learning rate?\n",
    "                 use_noise = True,         # Enable noise inputs?\n",
    "                 **kwargs\n",
    "                ):\n",
    "        super(G_NET, self).__init__()\n",
    "        self.nf = lambda res: min(int(fmap_base / (2.0 ** (res * fmap_decay))), fmap_max)  # number of features\n",
    "        self.structure = structure\n",
    "        self.resolution_log2 = int(np.log2(config.RESOLUTION))\n",
    "        \n",
    "        self.act = nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.bert_embedding = BERT_EMBEDDING()\n",
    "        self.ca_net = CA_NET()\n",
    "        self.mapping = G_MAPPING()\n",
    "        \n",
    "        \n",
    "        # noise input\n",
    "#         self.noise_inputs = []\n",
    "#         for layer_idx in range(num_layers):\n",
    "#             res = layer_idx // 2 + 2\n",
    "#             shape = [1, 1, 2 ** res, 2 ** res]\n",
    "#             if config.CUDA:\n",
    "#                 self.noise_inputs.append(torch.randn(*shape).to('cuda'))\n",
    "#             else:\n",
    "#                 self.noise_inputs.append(torch.randn(*shape))\n",
    "\n",
    "    \n",
    "        ## first layer\n",
    "        self.x = nn.Parameter(torch.ones(1, self.nf(2), 4, 4))\n",
    "        self.bias = nn.Parameter(torch.ones(self.nf(2)))\n",
    "        self.adaIn1 = Layer_Epilogue(self.nf(2),\n",
    "                                     4,\n",
    "                                     use_attn = use_attn,\n",
    "                                     use_wscale = use_wscale,\n",
    "                                     use_noise = use_noise,\n",
    "                                     use_pixel_norm = use_pixel_norm,\n",
    "                                     use_instance_norm = use_instance_norm\n",
    "                                    )\n",
    "        self.conv1  = Conv2d(self.nf(2), self.nf(2),\n",
    "                             kernel_size=3, use_wscale=use_wscale)\n",
    "        self.adaIn2 = Layer_Epilogue(self.nf(2),\n",
    "                                     4,\n",
    "                                     use_attn = use_attn,\n",
    "                                     use_wscale = use_wscale,\n",
    "                                     use_noise = use_noise,\n",
    "                                     use_pixel_norm = use_pixel_norm,\n",
    "                                     use_instance_norm = use_instance_norm\n",
    "                                    )\n",
    "        \n",
    "        ## remaining layers\n",
    "        self.generator = nn.ModuleList()\n",
    "        for log2_res in range(3, self.resolution_log2+1):\n",
    "            self.generator.append(G_BLOCK(log2_res))\n",
    "        \n",
    "        ## to image\n",
    "        self.torgb = Conv2d(self.nf(self.resolution_log2), out_channels, kernel_size=1, gain=1, use_wscale=use_wscale)\n",
    "        \n",
    "    def forward(self, text, z_code):\n",
    "        '''\n",
    "        Inputs:\n",
    "        \n",
    "        Outputs:\n",
    "            \n",
    "        '''\n",
    "        encoded_layers, pooled_layer = self.bert_embedding(text)\n",
    "        c_code, _, _ = self.ca_net(pooled_layer)\n",
    "        w_code = self.mapping(c_code, z_code)\n",
    "        x = self.x.expand(z_code.size(0), -1, -1, -1)\n",
    "        x = x + self.bias.view(1, -1, 1, 1)\n",
    "        x = self.act(x)\n",
    "        x = self.adaIn1(x, w_code, encoded_layers)\n",
    "        x = self.conv1(x)\n",
    "        x = self.adaIn2(x, w_code, encoded_layers)\n",
    "        \n",
    "        for block in self.generator:\n",
    "            x = block(x, w_code, encoded_layers)\n",
    "        x = self.torgb(x)\n",
    "        \n",
    "        return x, encoded_layers, pooled_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 4294967296 bytes. Error code 12 (Cannot allocate memory)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-ace0f80369e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mg_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG_NET\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-f6f7482a12ef>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fmap_base, out_channels, structure, fmap_max, fmap_decay, f, use_attn, use_pixel_norm, use_instance_norm, use_wscale, use_noise, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlog2_res\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolution_log2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_BLOCK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog2_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m## to image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-75342d97c0bb>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, log2_res, use_attn, use_wscale, use_noise, use_pixel_norm, use_instance_norm, f, factor, fmap_base, fmap_decay, fmap_max, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m                                      \u001b[0muse_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_noise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                                      \u001b[0muse_pixel_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_pixel_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                                      \u001b[0muse_instance_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_instance_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                                     )\n\u001b[1;32m     47\u001b[0m         self.conv1  = Conv2d(self.channel, self.channel,\n",
      "\u001b[0;32m<ipython-input-21-8ac00ea50f53>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, channels, res, use_attn, use_wscale, use_noise, use_pixel_norm, use_instance_norm, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mApply_Style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-68aac553886e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, channels, res, use_attn, use_wscale, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_attn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_attn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlobalAttentionGeneral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             self.fc = FC(self.w_dim + self.a_dim,\n\u001b[1;32m     18\u001b[0m                          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-6ad9d967ae81>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, channels, res)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matt_fc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mih\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplyMask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 4294967296 bytes. Error code 12 (Cannot allocate memory)\n"
     ]
    }
   ],
   "source": [
    "## test\n",
    "g_net = G_NET()\n",
    "out, encoded_layers, pooled_layer = g_net(text, z_code)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D_NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D_BLOCK(nn.Module):\n",
    "    def __init__(self,\n",
    "                 log2_res,           # Current Resolution,  log_2(resolution)...3\n",
    "                 f=None,             # (Huge overload, if you dont have enough resouces, please pass it as `f = None`)Low-pass filter to apply when resampling activations. None = no filtering.\n",
    "                 factor=2,           # upsample factor.\n",
    "                 fmap_base=8192,     # Overall multiplier for the number of feature maps.\n",
    "                 fmap_decay=1.0,     # log2 feature map reduction when doubling the resolution.\n",
    "                 fmap_max=512,       # Maximum number of feature maps in any layer.\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super(D_BLOCK, self).__init__()\n",
    "        self.nf = lambda res: min(int(fmap_base / (2.0 ** (res * fmap_decay))), fmap_max)\n",
    "        \n",
    "        self.act = nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.blur2d = Blur2d(f)\n",
    "        self.log2_res = log2_res\n",
    "        \n",
    "        self.channel = self.nf(self.log2_res)\n",
    "        \n",
    "        if self.nf(self.log2_res-1) == self.channel:\n",
    "            self.downsample = nn.AvgPool2d(2)\n",
    "        else:\n",
    "            self.downsample = nn.Conv2d(self.channel, self.nf(self.log2_res-1), kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv = nn.Conv2d(self.channel, self.channel, kernel_size=3, padding=(1,1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        '''\n",
    "        x = self.act(self.conv(x))\n",
    "        x = self.act(self.downsample(self.blur2d(x)))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "d_block = D_BLOCK(3)\n",
    "image = torch.rand(3, 512, 8, 8)\n",
    "d_out = d_block(image)\n",
    "print(d_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D_NET(nn.Module):\n",
    "    def __init__(self,\n",
    "                 fmap_base = 8192,         # Overall multiplier for the number of feature maps\n",
    "                 out_channels = 3,         # Number of output image colors\n",
    "                 structure = 'fixed',      # 'fixed' = no progressive growing, 'linear' = human-readable, 'recursive' = efficient\n",
    "                 fmap_max = 512,           # Maximum inumber of feature maps in any layer\n",
    "                 fmap_decay = 1.0,         # log2 feature map reduction when doubling the resolution\n",
    "                 f=None,                   # (Huge overload, if you dont have enough resouces, please pass it as `f = None`)Low-pass filter to apply when resampling activations. None = no filtering.\n",
    "                 **kwargs\n",
    "                ):\n",
    "        super(D_NET, self).__init__()\n",
    "        self.nf = lambda res: min(int(fmap_base / (2.0 ** (res * fmap_decay))), fmap_max)  # number of features\n",
    "        self.structure = structure\n",
    "        self.resolution_log2 = int(np.log2(config.RESOLUTION))\n",
    "        self.act = nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "        self.fromrgb = nn.Conv2d(out_channels, self.nf(self.resolution_log2), kernel_size=1)\n",
    "        \n",
    "        self.blur2d = Blur2d(f)\n",
    "        \n",
    "        self.discriminator = nn.ModuleList()\n",
    "        for log2_res in range(self.resolution_log2, 2, -1):\n",
    "            self.discriminator.append(D_BLOCK(log2_res))\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(self.nf(2), self.nf(1), kernel_size=3, padding=(1, 1))\n",
    "        self.fc1 = nn.Linear(fmap_base, int(fmap_base / 4))\n",
    "        self.fc2 = nn.Linear(int(fmap_base / 4), 1)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        '''\n",
    "        x = self.act(self.fromrgb(x))\n",
    "        for block in self.discriminator:\n",
    "            x = block(x)\n",
    "        x = self.act(self.conv_last(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "d_net = D_NET()\n",
    "image = torch.rand(3, 3, 512, 512)\n",
    "out = d_net(image)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## overal test\n",
    "text = ['It is a big dog.', \"I'm a boy\", \"It's the end. Don't continue.\"]\n",
    "z_code = torch.rand(3, config.Z_DIM)\n",
    "g_net = G_NET()\n",
    "d_net = D_NET()\n",
    "image,_,_ = g_net(text, z_code)\n",
    "print(image.shape)\n",
    "out = d_net(image)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
