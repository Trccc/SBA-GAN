%%%%%%%% ICML 2019 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2019} with \usepackage[nohyperref]{icml2019} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2020}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2020}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Proposal of Implementation and Improvement of StyleGAN}

\begin{document}

\twocolumn[
\icmltitle{Proposal of Implementation and Improvement of StyleGAN}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2019
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Fei Zheng fz2277}{equal,to}
\icmlauthor{Chirong Zhang cz2533}{equal,to}
\icmlauthor{Xiaoxi Zhao xz2740}{equal,to}
\end{icmlauthorlist}

\icmlaffiliation{to}{Department of Statistics, Columbia University, New York, USA}


\icmlcorrespondingauthor{Iddo Drori}{id2305@columbia.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

% \begin{abstract}
% This document provides a basic paper template and submission guidelines.
% Abstracts must be a single paragraph, ideally between 4--6 sentences long.
% Gross violations will trigger corrections at the camera-ready phase.
% \end{abstract}

\section{Previous work and references}

The Style-Based Generator Architecture for Cenerative Adversarial Networks (GAN)\cite{stylegan} has proposed a new generator architecture for GAN using style transfer techniques\cite{styletransferog}. This new architecture disentangles the latent factors of variation, which is one of the main limitations of ProGAN\cite{progan}. Here are the main breakthroughs:

\begin{itemize}
\item A style-based generator with unsupervised separation of high-level attributes 
\item Scale-specific control of synthesis
\item Generator starts from trainable constants and embeds the input latent code into an intermediate latent space which better disentangles the features
\item Two new metrics to quantify the disentanglement: perceptual path length and linear separability
\item Present a new dataset of human faces with wider variation and higher quality
\end{itemize}

\section{New Problem Proposal: Style Mixing}

The Style GAN\cite{stylegan} employ mixing regularization to localize the styles and operate style mixing with two latent codes $Z_1, z_2$. 

Instead of just mixing two styles, we want to run 3 latent codes through the mapping network to mix multiple styles in our generator. We will have the corresponding $w_1, w_2 , w_3$ applied between different crossover points. 

Initially, we choose three number of latent codes because coarse spatial resolutions $(4^2-8^2)$ brings high-level aspects such as pose, face shape, as middle resolutions $(16^2-32^2)$ brings hair style, eye open/closed and fine styles $(64^2-1024^2)$ brings color scheme and other microstructure.

We will first apply this style mixing to human faces and then apply it to animal datasets as well, to answer what will my cats look like if they become dogs and to generate photos of potential new breeds/species.

Then if time permits, we will further try to raise the number of styles and apply this to other datasets as well. (food, art) We will also try to extract one single aspect (e.g. nose) and only stylize that specific aspect. (e.g. change one's nose into a Joker's nose style)

\subsection{Evaluation Criteria}

We will use Frechet inception distance (FID)\cite{fid}for various generator designs (lower is better) ans also for each generated photos, we will calculate the style loss \cite{styletransferog}. 

\subsection{Dataset}

We will first apply our style mixing on a dataset of human faces (Flickr-Faces-HQ, FFHQ) \footnote{ https://github.com/NVlabs/stylegan}, then apply on the animals dataset \footnote{http://www.robots.ox.ac.uk/~vgg/data/pets/} \footnote{https://www.kaggle.com/alessiocorrado99/animals10}


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2020}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DELETE THIS PART. DO NOT PLACE CONTENT AFTER THE REFERENCES!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \appendix
% \section{Do \emph{not} have an appendix here}

% \textbf{\emph{Do not put content after the references.}}
% %
% Put anything that you might normally include after the references in a separate
% supplementary file.

% We recommend that you build supplementary material in a separate document.
% If you must create one PDF and cut it up, please be careful to use a tool that
% doesn't alter the margins, and that doesn't aggressively rewrite the PDF file.
% pdftk usually works fine. 

% \textbf{Please do not use Apple's preview to cut off supplementary material.} In
% previous years it has altered margins, and created headaches at the camera-ready
% stage. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
